{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a585efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytesseract\n",
    "#!pip install easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8fb4ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#import constants\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import time\n",
    "from time import time as timer\n",
    "#from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "#import requests\n",
    "import urllib\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b1e3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(image_link, save_folder, retries=3, delay=3):\n",
    "    if not isinstance(image_link, str):\n",
    "        return\n",
    "\n",
    "    filename = Path(image_link).name\n",
    "    image_save_path = os.path.join(save_folder, filename)\n",
    "\n",
    "    if os.path.exists(image_save_path):\n",
    "        return\n",
    "\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(image_link, image_save_path)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    create_placeholder_image(image_save_path) #Create a black placeholder image for invalid links/images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "268dfd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholder_image(image_save_path):\n",
    "    try:\n",
    "        placeholder_image = Image.new('RGB', (100, 100), color='black')\n",
    "        placeholder_image.save(image_save_path)\n",
    "    except Exception as e:\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "902f4763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 : Normalization\n",
    "# Normalization of the image without adaptive threshold\n",
    "\n",
    "def step_1(image_path):\n",
    "    try:\n",
    "        # Read color image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Normalizing the color image (keeping 3 channels)\n",
    "        normalized_color_image = cv2.normalize(\n",
    "            image, \n",
    "            None, \n",
    "            alpha=0, \n",
    "            beta=255, \n",
    "            norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "        # Writing the output back to the input file source\n",
    "        cv2.imwrite(image_path, normalized_color_image)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2729ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #STEP 3 : IMAGE SCALING (scaling all images to have DPI of 300)\n",
    "# def set_image_dpi(image_path):\n",
    "    \n",
    "#     try : \n",
    "#         # Open the image\n",
    "#         im = Image.open(image_path)\n",
    "\n",
    "#         # Resize the image\n",
    "#         length_x, width_y = im.size\n",
    "#         factor = min(1, float(1024.0 / length_x))\n",
    "#         size = int(factor * length_x), int(factor * width_y)\n",
    "#         im_resized = im.resize(size, Image.Resampling.LANCZOS)\n",
    "\n",
    "#         # Writing output back to the input file source\n",
    "#         im_resized.save(image_path, dpi=(300, 300))\n",
    "    \n",
    "#     except Exception as e :\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a1139288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 : Noise Removal - removing small dots/patches w high intensity compared to the rest of the image\n",
    "def remove_noise(image_path):\n",
    "    # Read the image from the file path\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Check if the image was loaded correctly\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image could not be loaded from {image_path}.\")\n",
    "    \n",
    "    # Apply the denoising function\n",
    "    denoised_img = cv2.fastNlMeansDenoisingColored(image, None, 4, 0, 11, 19)\n",
    "    \n",
    "    # Writing output back to the input file source\n",
    "    cv2.imwrite(image_path, denoised_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6421ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 : Grayscale + Threshold\n",
    "# Using automatic threshold decider to convert an already grayscale image to increase contrast\n",
    "\n",
    "def thresholding(image_path):\n",
    "    image = cv2.imread(image_path)  # Read the image\n",
    "    grayscale_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert the image to grayscale\n",
    "    cv2.imwrite(image_path, grayscale_img)  # Save the grayscale image\n",
    "    \n",
    "    #threshold_img = cv2.threshold(grayscale_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]  # Apply Otsu's threshold\n",
    "    #cv2.imwrite(image_path, threshold_img)  # Save the thresholded image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a245ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to deskew an image\n",
    "def deskew_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Apply edge detection (Canny)\n",
    "    edges = cv2.Canny(image, 50, 150, apertureSize=3)\n",
    "\n",
    "    # Apply Hough Line Transform to detect lines in the image\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 100, minLineLength=100, maxLineGap=10)\n",
    "\n",
    "    # Calculate the angle of each line\n",
    "    angles = []\n",
    "    \n",
    "    try :\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            angle = np.degrees(np.arctan2(y2 - y1, x2 - x1))\n",
    "            angles.append(angle)\n",
    "\n",
    "        # Find the median angle (to handle outliers)\n",
    "        median_angle = np.median(angles)\n",
    "\n",
    "        # Rotate the image to deskew it\n",
    "        (h, w) = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, median_angle, 1.0)\n",
    "        deskewed_image = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "        cv2.imwrite(image_path, deskewed_image)\n",
    "    except :\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67a1184c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ADITI CODE\n",
    "\n",
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# allowed_units = {\n",
    "#     'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "#     'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "#     'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "#     'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "#     'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "#     'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "#     'wattage': {'kilowatt', 'watt'},\n",
    "#     'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', \n",
    "#                     'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "# }\n",
    "\n",
    "# unit_prefixes = {\n",
    "#     'cm': 'centimetre', 'mm': 'millimetre', 'm': 'metre', 'kg': 'kilogram',\n",
    "#     'g': 'gram', 'mg': 'milligram', 'µg': 'microgram', 'l': 'litre',\n",
    "#     'ml': 'millilitre', 'cl': 'centilitre', 'dl': 'decilitre', 'µl': 'microlitre',\n",
    "#     'oz': 'ounce', 'lb': 'pound', 'ft': 'foot', 'in': 'inch', 'yd': 'yard',\n",
    "#     'gal': 'gallon', 'pt': 'pint', 'qt': 'quart', 'kv': 'kilovolt',\n",
    "#     'mv': 'millivolt', 'v': 'volt', 'kw': 'kilowatt', 'w': 'watt'\n",
    "# }\n",
    "\n",
    "# spl_plurals = {\n",
    "#     'inches': 'inch', 'feet': 'foot', 'pounds': 'pound', 'ounces': 'ounce',\n",
    "#     'liters': 'litre', 'cubic feet': 'cubic foot', 'cubic inches': 'cubic inch',\n",
    "#     'fluid ounces': 'fluid ounce', 'meters': 'metre', 'grams': 'gram'\n",
    "# }\n",
    "\n",
    "\n",
    "# def normalize_unit(unit, entity_name):\n",
    "#     unit = unit.lower()\n",
    "#     # check for spl plurals, then remove s for normal plurals then give out whatever entity init\n",
    "#     if unit in spl_plurals:\n",
    "#         normalized = spl_plurals[unit]\n",
    "#         if normalized in allowed_units.get(entity_name, set()):\n",
    "#             return normalized\n",
    "    \n",
    "#     singular = unit.rstrip('s')\n",
    "#     if singular in allowed_units.get(entity_name, set()):\n",
    "#         return singular\n",
    "    \n",
    "#     for prefix, full_form in unit_prefixes.items():\n",
    "#         if unit.startswith(prefix.lower()) and full_form in allowed_units.get(entity_name, set()):\n",
    "#             return full_form\n",
    "    \n",
    "#     return \"\"\n",
    "\n",
    "# # gets number + unit -> gives unit to normalize\n",
    "# def extract_measurement(text, entity_name):\n",
    "#     doc = nlp(text)\n",
    "    \n",
    "#     quantity = None\n",
    "#     unit = None\n",
    "    \n",
    "#     for token in doc:\n",
    "#         if token.like_num and not quantity:\n",
    "#             quantity = token.text\n",
    "#         elif not unit:\n",
    "#             unitpart = token.text.lower()\n",
    "#             normalized_unit = normalize_unit(unitpart, entity_name)\n",
    "#             if normalized_unit:\n",
    "#                 unit = normalized_unit\n",
    "    \n",
    "    \n",
    "#     if quantity and unit:\n",
    "#         result = f\"{quantity} {unit}\"\n",
    "#         return result\n",
    "    \n",
    "#     return \"\"\n",
    "\n",
    "# # # dummy\n",
    "# # test_cases = [\n",
    "# #     ([(189, 75), (469, 75), (469, 165), (189, 165)], '200 pounds', 'item_weight'),\n",
    "# #     ([(100, 100), (200, 100), (200, 200)], '150 inches', 'height'),\n",
    "# #     ([(50, 50), (150, 50), (150, 150)], '2.5 liters', 'item_volume'),\n",
    "# #     ([(75, 75), (175, 75), (175, 175)], '500 inch', 'item_volume'), \n",
    "# #     ([(25, 25), (125, 25), (125, 125)], '6 ft', 'width'),\n",
    "# #     ([(10, 10), (110, 10), (110, 110)], '1000 millivolts', 'voltage')\n",
    "# # ]\n",
    "\n",
    "# # # Process test cases\n",
    "# # for bbox, text, entity in test_cases:\n",
    "# #     prediction = extract_measurement(text, entity)\n",
    "# #     print(f\"Final Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "172f1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Entity-to-unit mappings\n",
    "entity_unit_map = {\n",
    "    \"width\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n",
    "    \"depth\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n",
    "    \"height\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n",
    "    \"item_weight\": {\"milligram\", \"kilogram\", \"microgram\", \"gram\", \"ounce\", \"ton\", \"pound\"},\n",
    "    \"maximum_weight_recommendation\": {\"milligram\", \"kilogram\", \"microgram\", \"gram\", \"ounce\", \"ton\", \"pound\"},\n",
    "    \"voltage\": {\"millivolt\", \"kilovolt\", \"volt\"},\n",
    "    \"wattage\": {\"kilowatt\", \"watt\"},\n",
    "    \"item_volume\": {\"cubic foot\", \"microlitre\", \"cup\", \"fluid ounce\", \"centilitre\", \"imperial gallon\", \"pint\", \"decilitre\", \"litre\", \"millilitre\", \"quart\", \"cubic inch\", \"gallon\"}\n",
    "}\n",
    "\n",
    "# Common unit prefixes and their full forms\n",
    "unit_prefixes = {\n",
    "    'cm': 'centimetre',\n",
    "    'mm': 'millimetre',\n",
    "    'm': 'metre',\n",
    "    'kg': 'kilogram',\n",
    "    'g': 'gram',\n",
    "    'mg': 'milligram',\n",
    "    'µg': 'microgram',\n",
    "    'l': 'litre',\n",
    "    'ml': 'millilitre',\n",
    "    'cl': 'centilitre',\n",
    "    'dl': 'decilitre',\n",
    "    'µl': 'microlitre',\n",
    "    'oz': 'ounce',\n",
    "    'lb': 'pound',\n",
    "    'ft': 'foot',\n",
    "    'in': 'inch',\n",
    "    'yd': 'yard',\n",
    "    'gal': 'gallon',\n",
    "    'pt': 'pint',\n",
    "    'qt': 'quart',\n",
    "    'kv': 'kilovolt',\n",
    "    'mv': 'millivolt',\n",
    "    'v': 'volt',\n",
    "    'kw': 'kilowatt',\n",
    "    'w': 'watt'\n",
    "}\n",
    "\n",
    "# Normalize the unit to match the full form and correct plural cases\n",
    "def normalize_unit(unit, entity_name):\n",
    "    unit = unit.lower().strip()\n",
    "    \n",
    "    # Handle special plural cases\n",
    "    special_cases = {\n",
    "        'inches': 'inch',\n",
    "        'feet': 'foot',\n",
    "        'pounds': 'pound',\n",
    "        'ounces': 'ounce',\n",
    "        'liters': 'litre',\n",
    "        'cubic feet': 'cubic foot',\n",
    "        'cubic inches': 'cubic inch',\n",
    "        'fluid ounces': 'fluid ounce',\n",
    "        'meters': 'metre',\n",
    "        'grams': 'gram'\n",
    "    }\n",
    "    \n",
    "    if unit in special_cases:\n",
    "        normalized_unit = special_cases[unit]\n",
    "        if normalized_unit in entity_unit_map.get(entity_name, set()):\n",
    "            return normalized_unit\n",
    "\n",
    "    # Remove plural 's' and check\n",
    "    unit = unit.rstrip('s')\n",
    "\n",
    "    if unit in entity_unit_map.get(entity_name, set()):\n",
    "        return unit\n",
    "\n",
    "    # Check for unit prefixes (e.g., 'kg' -> 'kilogram')\n",
    "    for prefix, full_form in unit_prefixes.items():\n",
    "        if unit == prefix and full_form in entity_unit_map.get(entity_name, set()):\n",
    "            return full_form\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "# Extract quantities and units using regex\n",
    "def extract_units_and_quantities(text):\n",
    "    # Regex to capture numbers followed by units (with or without spaces)\n",
    "    matches = re.findall(r'(\\d+(?:\\.\\d+)?)\\s*([a-zA-Zµ]+)', text)\n",
    "    return matches\n",
    "\n",
    "# Extract quantity and unit for a given entity\n",
    "def extract_quantity_for_entity(text, entity_name):\n",
    "    # Apply spacy NLP model to process text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract relevant tokens (numbers and units)\n",
    "    units_and_quantities = extract_units_and_quantities(text)\n",
    "    \n",
    "    for quantity, unit in units_and_quantities:\n",
    "        # Normalize the unit and check if it's valid for the given entity\n",
    "        normalized_unit = normalize_unit(unit, entity_name)\n",
    "        if normalized_unit:\n",
    "            return f\"{quantity} {normalized_unit}\"\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "# Process the text for a specific entity\n",
    "def process_text(text, entity_name):\n",
    "    prediction = extract_quantity_for_entity(text, entity_name)\n",
    "    if prediction:\n",
    "        return f\"{entity_name}: {prediction}\"\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0951014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images/314oOH7ICvS.jpg\n",
      "30cm/\n",
      "60cm\n",
      "cm\n",
      "151,5\n",
      "\n",
      "./images/314oOH7ICvS.jpg\n",
      "151,5cm\n",
      "30 cm\n",
      "cm\n",
      "\n",
      "./images/314p2NKHdLL.jpg\n",
      "REBAR TYING MACHINE\n",
      "\n",
      "./images/314p2NKHdLL.jpg\n",
      "REBAR TYING MACHINE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ORIGINAL WORKING PADDLE-----------------------------------------\n",
    "\n",
    "import csv\n",
    "from paddleocr import PaddleOCR, draw_ocr\n",
    "import logging\n",
    "\n",
    "count = 0\n",
    "with open(\"./dataset/test.csv\", 'r') as fh :\n",
    "    reader = csv.reader(fh)\n",
    "    count = 0\n",
    "    for row in reader :\n",
    "        if count != 0 :\n",
    "            index, link, product_code, reqd_entity = row[0], row[1], row[2], row[3]\n",
    "            \n",
    "            download_image(link, \"./images/\")\n",
    "            image_name = link.split(\"/\")[5]\n",
    "            img_path = './images/' + image_name\n",
    "            \n",
    "            print(img_path)\n",
    "            # Performing the preprocessing\n",
    "            step_1(img_path)\n",
    "            set_image_dpi(img_path)\n",
    "            remove_noise(img_path)\n",
    "            thresholding(img_path)\n",
    "            deskew_image(img_path)\n",
    "    \n",
    "            logging.getLogger(\"ppocr\").setLevel(logging.ERROR)\n",
    "            ocr = PaddleOCR(use_gpu = True, use_angle_cls=True, lang='en', show_logs = False) # Initialize PaddleOCR            \n",
    "            result = ocr.ocr(img_path)  # Perform OCR on the image\n",
    "            \n",
    "\n",
    "            # Print the result\n",
    "#             print(f\"\\n__________________{img_path}____________________\\n\")\n",
    "#             print(f\"\\n__________________{reqd_entity}____________________\\n\")\n",
    "#             print(f\"\\n__________________{exp_out}____________________\\n\")\n",
    "            \n",
    "            text_input = \"\"\"\"\"\"\n",
    "            for line in result:\n",
    "                for word_info in line:\n",
    "                    word = word_info[1][0]  # Extract the word\n",
    "                    text_input += word + \"\\n\"                                           \n",
    "                        \n",
    "            print(text_input)\n",
    "            \n",
    "            # Process the input text for all entities in entity_unit_map and print only relevant ones\n",
    "            output = process_text(text_input, reqd_entity)\n",
    "            data = [index, output]\n",
    "            \n",
    "            with open(\"test_out.csv\", 'a') as fh :\n",
    "                writer = csv.writer(fh)\n",
    "                writer.writerow(data)\n",
    "\n",
    "#             if output :\n",
    "#                 print(output)\n",
    "#             else :\n",
    "#                 print(\"NOTHING\")\n",
    "            \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97121e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ORIGINAL WORKING PADDLE-----------------------------------------\n",
    "\n",
    "# import csv\n",
    "# from paddleocr import PaddleOCR, draw_ocr\n",
    "# import logging\n",
    "\n",
    "# count = 0\n",
    "# with open(\"./dataset/train.csv\", 'r') as fh :\n",
    "#     reader = csv.reader(fh)\n",
    "#     count = 0\n",
    "#     for row in reader :\n",
    "#         if count != 0 and 4 <= count <= 7:\n",
    "#             link = row[0]\n",
    "#             reqd_entity = row[2]\n",
    "#             exp_out = row[3]\n",
    "            \n",
    "#             download_image(link, \"./images/\")\n",
    "#             image_name = link.split(\"/\")[5]\n",
    "#             img_path = './images/' + image_name\n",
    "            \n",
    "#             # Performing the preprocessing\n",
    "#             step_1(img_path)\n",
    "#             set_image_dpi(img_path)\n",
    "#             remove_noise(img_path)\n",
    "#             thresholding(img_path)\n",
    "#             deskew_image(img_path)\n",
    "    \n",
    "#             logging.getLogger(\"ppocr\").setLevel(logging.ERROR)\n",
    "#             ocr = PaddleOCR(use_gpu = True, use_angle_cls=True, lang='en', show_logs = False) # Initialize PaddleOCR            \n",
    "#             result = ocr.ocr(img_path)  # Perform OCR on the image\n",
    "            \n",
    "\n",
    "#             # Print the result\n",
    "#             print(f\"\\n__________________{img_path}____________________\\n\")\n",
    "#             print(f\"\\n__________________{reqd_entity}____________________\\n\")\n",
    "#             print(f\"\\n__________________{exp_out}____________________\\n\")\n",
    "            \n",
    "#             text_input = ''\n",
    "#             for line in result:\n",
    "#                 for word_info in line:\n",
    "#                     word = word_info[1][0]  # Extract the word\n",
    "#                     #text_input += word + \"\\n\"\n",
    "                    \n",
    "#                     words_to_pass = []\n",
    "#                     if word.isalpha() :\n",
    "#                         pass\n",
    "                    \n",
    "#                     elif word.isalnum() :\n",
    "#                         words_in_word = word.split()\n",
    "#                         for j in range(len(words_in_word)) :\n",
    "#                             i = words_in_word[j]\n",
    "                            \n",
    "#                             # like 200ml\n",
    "#                             if i.isalnum() :\n",
    "#                                 words_to_pass.append(i)\n",
    "                                \n",
    "#                             # like 200, ml\n",
    "#                             elif i.isalpha() :\n",
    "#                                 try :\n",
    "#                                     s = i + words_in_word[j + 1]\n",
    "#                                 except :\n",
    "#                                     pass\n",
    "                            \n",
    "                                \n",
    "                            \n",
    "                                \n",
    "                        \n",
    "#             print(text_input)\n",
    "            \n",
    "#             # Process the input text for all entities in entity_unit_map and print only relevant ones\n",
    "#             output = process_text(text_input, reqd_entity)\n",
    "#             # print(output)\n",
    "# #             if output:\n",
    "# #                 print(output)\n",
    "# #             else : \n",
    "# #                 ''\n",
    "            \n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7c4bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocr = PaddleOCR(use_angle_cls=True, lang='en', show_logs = False) # Initialize PaddleOCR\n",
    "# img_path = \"/home/suppra/Desktop/Amazon_ML/student_resource 3/images/hazy.jpeg\"# Path to your image\n",
    "\n",
    "# result = ocr.ocr(img_path)  # Perform OCR on the image\n",
    "\n",
    "\n",
    "# # Print the result\n",
    "# print(f\"\\n__________________{img_path}____________________\\n\")\n",
    "# for line in result:\n",
    "#     for word_info in line:\n",
    "#         word = word_info[1][0]  # Extract the word\n",
    "#         print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_kernel",
   "language": "python",
   "name": "ocr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
